\chapter{Ergebnisse}
\label{ch:ergebnisse}
In diesem Kapitel werden die interessantesten Ergebnisse \cite{githubRepo}, die beim Training erzielt wurden, vorgestellt und visualisiert. Der Übersichtlichkeit halber befindet sich ein Großteil der Graphen im Anhang (s. Anhang \ref{ch:Anhang_Scatterplots}) und wird im weiteren Verlauf dieses Kapitels lediglich beschrieben und referenziert.
Eine detaillierte tabellarische Auflistung der finalen Accuracies auf den Testdaten jeweils über alle 5 bzw. 3 Folds gemittelt befindet sich im Anhang (s. Anhang \ref{ch:Anhang_Tabellen}).

\section{Unterschied zwischen OvO und OvA}
\label{ch:ergebnisseOvOOvA}
In Abbildungen \ref{fig:ScatterplotRS} und \ref{fig:ScatterplotRF} sowie im Anhang (s. Anhang \ref{ch:Anhang_ScatterplotsAccuracies}) befindet sich jeweils pro Datensatz und verwendeter Klassenanzahl ein Graph. Dieser besteht aus 5 mit dicken Linien voneinander abgegrenzten Bereichen, in denen jeweils nur ein Anteil von 10, 20, 50, 80 oder 100 Prozent der Trainingsdaten zum Training verwendet wurde. Innerhalb einem dieser 5 Bereiche existieren 3 mit dünnen Linien voneinander abgegrenzte Regionen. Diese stehen für die 3 verwendeten Frameworks: TensorFlow \cite{tensorflow} in der Version 1.13.1 und 2.4.1 sowie PyTorch \cite{pytorch} 1.9.0 in dieser Reihenfolge. Die hervorgehobene Markierung steht für den Mittelwert der jeweiligen 5- bzw. 3-Fold Cross Validation während die kleinen Punkte die Ergebnisse auf den einzelnen 5 bzw. 3 Folds repräsentieren.

In den Gruppierungen von Graphen (Abb. \ref{fig:ScatterplotRS}, \ref{fig:ScatterplotRF} und Anhang \ref{ch:Anhang_ScatterplotsAccuracies}) sind die einzelnen Datensätze farblich hervorgehoben, sodass in Beziehung miteinander stehende Graphen leichter erkennbar sind.\\

Alle Trainingsdurchläufe für einen Datensatz in Kombination mit einer Klassenanzahl und Trainsize verwenden in allen Fällen die exakt gleichen Trainingsdaten. So werden beispielsweise bei Fold 1 für Agrilplant3 in Resnet Scratch unter Verwendung von TensorFlow \cite{tensorflow} 1.13.1 (s. Abb. \ref{fig:ScatterplotRS}) die genau gleichen Trainingsdaten verwendet wie bei Fold 1 von Agrilplant3 in Inception Finetune und PyTorch \cite{pytorch} als Framework (s. Abb. \ref{fig:ScatterplotIF} im Anhang \ref{ch:Anhang_ScatterplotsAccuracies}). Die genaue Aufteilung der Bilder im Datensatz in die verschiedenen Teilmengen ist in dem GitHub-Repository zu dieser Bachelorarbeit \cite{githubRepo} aufgelistet, sodass die von mir erzielten Ergebnisse mit Ausnahme der zufälligen Initialisierung der Gewichte im Netz sowie der zufällig auftretenden Data-Augmentation möglichst gut reproduzierbar sind.\\

Durch diese Wiederverwendung der Teilmengen der Datensätze werden zufällig auftretende Schwankungen, die besonders bei niedrigen Trainsizes mit ohnehin kleinen Datensätzen auftreten, vermieden und eine gute Vergleichbarkeit der Ergebnisse wird sichergestellt.\\

Der Datensatz Cifar10 \cite{cifar10} wird, wie in Kapitel \ref{ch:methodik_datensaetzeCifar10} bereits beschrieben, nur bei Resnet Scratch verwendet. Außerdem wird für das InceptionV3-Netz mit den Änderungen von Pawara et al. \cite{pawaraWebsiteCode} nur TensorFlow \cite{tensorflow} in beiden Versionen als Framework verwendet, PyTorch \cite{pytorch} hingegen nicht. Somit existieren für Abb. \ref{fig:ScatterplotIPS} und \ref{fig:ScatterplotIPF} nur 2 Bereiche, einen je Version von TensorFlow \cite{tensorflow}, innerhalb der 5 mit dicken Linien voneinander abgegrenzten Regionen in den Scatterplots.


\subsection{ResNet Scratch}
\label{ch:ergebnisseOvOOvA-RS}
In Abbildung \ref{fig:ScatterplotRS} fällt auf, dass besonders für niedrige Trainsizes, also Prozentsätze der tatsächlich verwendeten Trainingsdaten, das OvO Klassifikationsschema für TensorFlow \cite{tensorflow} 1.13.1 und PyTorch \cite{pytorch} in den meisten Fällen bessere Ergebnisse produziert als OvA. Je höher der Anteil der verwendeten Trainingsdaten geht, umso mehr nähern sich die Accuracies der beiden Klassifikationsschemata aneinander an. Es zeichnet sich zudem der Trend ab, dass PyTorch \cite{pytorch} fast immer Ergebnisse produziert, die genau so gut oder besser als die beiden von TensorFlow \cite{tensorflow} sind.


Für Cifar10 \cite{cifar10} erlangt man allerdings genau gegenteilige Ergebnisse. Dort ist OvO immer deutlich schlechter als OvA und die Ergebnisse von PyTorch \cite{pytorch} liegen in fast allen Fällen deutlich unter denen von TensorFlow \cite{tensorflow}. Beide Versionen von TensorFlow \cite{tensorflow} sind in etwa gleich gut.

Bei allen anderen Datensätzen fällt außerdem auf, dass die Ergebnisse für OvA in der neueren Version 2.4.1 etwas besser geworden sind. Für OvO im Vergleich zu Version 1.13.1 ist die neue Version von TensorFlow \cite{tensorflow} fast immer deutlich schlechter. Dadurch nähern sich in Version 2.4.1 die Ergebnisse von OvO und OvA aneinander an und der Unterschied zwischen den beiden Klassifikationsschemata, wie er in Version 2.4.1 existiert, verschwindet in den meisten Fällen nahezu komplett bzw. kehrt sich um, sodass OvA besser ist als OvO.

Besonders auffällig sind die Ausreißer für TensorFlow \cite{tensorflow} 2.4.1 bei den SwedishLeaves \cite{swedishLeaves} Datensätzen. Hierbei werden für niedrige Anzahlen an Klassen in Kombination mit einer kleinen Trainsize, also dem tatsächlich verwendeten Anteil an Trainingsdaten, mit beiden Klassifikationsschemata sehr schlechte Ergebnisse produziert, die teilweise nicht besser sind als zufälliges Raten.
\begin{figure}[H]
\begin{adjustbox}{width=\textwidth, center}
\includesvg{img/3_R-S}
\end{adjustbox}
\caption{Scatterplot der erzielten Accuracies mit beiden Klassifikationsschemata in allen 3 Frameworks für Resnet Scratch.}
\label{fig:ScatterplotRS}
\end{figure}


\subsection{Inception Scratch}
In der Abbildung \ref{fig:ScatterplotIS} zu InceptionV3 Scratch sind alle Frameworks ungefähr gleich gut, lediglich die in Kapitel \ref{ch:ergebnisseOvOOvA-RS} bereits beschriebenen Ausreißer bei \textit{swedishLeaves} \cite{swedishLeaves} treten hier in gleicher Form auf.

Der OvO Ansatz ist in allen Fällen gleich gut oder sogar besser als der OvA Ansatz, nur bei Agrilplant3 erzeugt das standardmäßige OvA Klassifikationsschema bessere Ergebnisse im Vergleich zu OvO.

Besonders die Tropic und Monkey \cite{pawaraWebsiteDatensaetze} Datensätze profitieren von dem OvO Klassifikationsschema bei niedrigeren Trainsizes in allen 3 Frameworks deutlich.


\subsection{Inception-Pawara Scratch}
Für das Netz InceptionV3 mit den Änderungen an den letzten Schichten aus dem Quellcode von Pawara et al. \cite{pawaraWebsiteCode} (vgl. Abb. \ref{fig:inceptionAenderungen}) ergeben sich nahezu identische Ergebnisse wie bei dem Standard InceptionV3 Netz ohne Änderungen. Vergleicht man Abbildung \ref{fig:ScatterplotIS} und \ref{fig:ScatterplotIPS} fällt auf, dass die Ergebnisse bei dem Netz Inception-Pawara, also mit den Änderungen von Pawara et al., geringfügig über denen von der Standard Inception Implementierung liegen. So beginnt beispielsweise die automatisch erzeugte Beschriftung der Y-Achsen bei \textit{Pawara-Monkey10} und \textit{Pawara-Tropic10} erst bei einem höheren Wert. Mit bloßem Auge lassen sich aber ansonsten kaum Unterschiede zur Standard InceptionV3 Variante erkennen.


\subsection{ResNet Finetune}
Für ResNet-50 Finetune (s. Abb. \ref{fig:ScatterplotRF}) fällt erneut auf, dass bei der neuen TensorFlow \cite{tensorflow} Version 2.4.1 bei beiden Klassifikationsschemata gleichermaßen starke Ausreißer nach unten auftreten, sodass die Genauigkeit der Klassifikation teilweise ähnlich schlecht wird wie pures Raten. Diese Ausreißer treten überraschenderweise nun sogar häufiger auf als bei ResNet-50 Scratch (vgl. Abb. \ref{fig:ScatterplotRS}), aber wieder nur bei kleineren Trainsizes und Datensätzen mit wenigen Trainingsdaten.

Ansonsten ist das standardmäßige OvA Klassifikationsschema bei einem Großteil der Ergebnisse deutlich besser als OvO. Alle 3 Frameworks produzieren mit Ausnahme der Ausreißer von TensorFlow \cite{tensorflow} 2.4.1 ähnlich gute Ergebnisse.
\begin{figure}[H]
\begin{adjustbox}{width=\textwidth, center}
\includesvg{img/3_R-F}
\end{adjustbox}
\caption{Scatterplot der erzielten Accuracies mit beiden Klassifikationsschemata in allen 3 Frameworks für Resnet Finetune.}
\label{fig:ScatterplotRF}
\end{figure}
\subsection{Inception Finetune}
Im Falle von InceptionV3 Finetune (s. Abb. \ref{fig:ScatterplotIF}) werden insgesamt sehr gute Accuracies von fast immer über 90 Prozent erzielt und alle 3 Frameworks produzieren gleich gute Ergebnisse. Anders als bei ResNet-50 Finetune (s. Abb. \ref{fig:ScatterplotRF}) treten die Ausreißer für TensorFlow \cite{tensorflow} 2.4.1 nur noch deutlich seltener auf und sind weniger stark ausgeprägt.

Insgesamt schwanken die Ergebnisse etwas. Für manche Datensätze wie z.B. Agrilplant10, Tropic10 und Tropic20 schneidet die OvA Klassifikation deutlich besser ab, bei Pawara-uMonkey10 und Tropic3 ist es jedoch genau umgekehrt.
Es lässt sich insgesamt kein Gewinner unter den beiden Klassifikationsschemata ausfindig machen. Die Unterschiede zwischen ihnen sind abhängig vom Datensatz und fallen eher gering aus. So unterscheiden sich die beiden Klassifikationsschemata um maximal 2 bis 3 Prozentpunkte, was aber durch den kleinen Wertebereich auf der Y-Achse und die damit einhergehende stark vergrößerte Ansicht der Werte nahe bei 100 Prozent gravierender wirkt als es eigentlich ist.

\subsection{Inception-Pawara Finetune}
Analog zu InceptionV3 Finetune lassen sich für Abbildung \ref{fig:ScatterplotIPF} mit Inception-Pawara Finetune ähnliche Aussagen treffen. Die Accuracies sind insgesamt sehr gut und liegen meistens deutlich über 90 Prozent.
Es fällt auf, dass die neue Version 2.4.1 von TensorFlow \cite{tensorflow} häufig schlechtere Ergebnisse produziert als die alte Version 1.13.1.

Im Gegensatz zu InceptionV3 Finetune lässt sich hier jedoch beobachten, dass der OvA Klassifikationsansatz in den meisten Fällen zu besseren Ergebnissen führt als der OvO Ansatz.




\section{Unterschiede der Trainingsdauer}
\label{ch:ergebnisseOvOOvA-Dauer}
Analog zu den in Kapitel \ref{ch:ergebnisseOvOOvA} gezeigten und beschriebenen Grafiken wird in diesem Kapitel die benötigte Trainingsdauer der verschiedenen Frameworks miteinander verglichen. Weitere Scatterplots neben Abbildung \ref{fig:ScatterplotRS-dauer} befinden sich im Anhang \ref{ch:Anhang_ScatterplotsDauer}.\\

Die Rohwerte der Trainingsdauer sind nicht repräsentativ, da zum Training verschiedene Grafikkarten-Modelle mit jeweils verschiedenen Leistungsfähigkeiten zur Verfügung standen (s. Kapitel \ref{ch:methodik_palma}). Deswegen hängt die Trainingsdauer zudem von der zum Training verwendeten Grafikkarte ab.
Um dem entgegen zu wirken und einen guten Vergleich zu ermöglichen, wurde die Trainingsdauer in den Scatterplots (s. Abb. \ref{ch:ergebnisseOvOOvA-Dauer} und Anhang \ref{ch:Anhang_ScatterplotsDauer}) auf das GPU-Modell \textit{RTX 2080} normalisiert. Dabei wird ein Faktor zur Normalisierung verwendet, der die Trainingsdauer von schnelleren Grafikkarten-Modellen länger macht und von langsameren Grafikkarten-Modellen verkürzt, sodass alle benötigten Trainingszeiten auf den Niveau einer RTX 2080 und damit miteinander vergleichbar sind (vgl. Tabelle \ref{tab:GPUNormalisierung}).
\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline 
GPU-Modell & GFLOPs (SP) & Normalisierungsfaktor\\
& & \\
\hline 
RTX 2080 & 11750 & $\frac{11750}{11750}=1$ \\
& & \\
\hline 
V100 & 14899 & $\frac{14899}{11750}=1,268$ \\
& & \\
\hline 
Titan RTX & 12441 & $\frac{12441}{11750} \approx 1,059$ \\
& & \\
\hline 
Titan XP & 10790 & $\frac{10790}{11750}\approx 0,918$ \\
& & \\
\hline 
\end{tabular} 
\caption{Leistungsfähigkeit und Normalisierungsfaktor für alle verfügbaren GPU-Modelle \cite{palma2GPUs}}
\label{tab:GPUNormalisierung}
\end{table}

Die Trainingsdauer (s. Abb. \ref{fig:ScatterplotRS-dauer}) steigt mit zunehmender Trainsize an, da dann mehr Trainingsdaten verarbeitet werden müssen.
Es fällt auf, dass beide Versionen von TensorFlow \cite{tensorflow} ziemlich genau gleich lange für das Training benötigen. Lediglich PyTorch \cite{pytorch} weicht teilweise stark von der Dauer der beiden TensorFlow \cite{tensorflow} Versionen ab. Bei sehr vielen Trainingsdaten ist PyTorch \cite{pytorch} wesentlich schneller als TensorFlow \cite{tensorflow}, bei wenigen Trainingsdaten dafür jedoch wesentlich langsamer.

Die benötigte Trainingszeit für OvO und OvA ist je Framework nahezu identisch, es kommt bei den verwendeten Anzahlen an Klassen trotz der größeren Ausgabeschicht der Netze und damit größeren Anzahl an Parametern im Netz nicht zu einem erkennbaren Mehraufwand für das Training mit dem OvO Klassifikationsschema.

Für Datensätze mit sehr vielen Bildern fällt auf, dass die Trainingszeit für beide Klassifikationsschemata als natürliche Konsequenz deutlich länger ist, da mehr Trainingsdaten in das Training mit einbezogen werden müssen. So benötigt beispielsweise das Training mit Cifar10 \cite{cifar10} ungefähr 10-mal so lange wie das Training mit Tropic20 \cite{pawaraWebsiteDatensaetze}, es sind dort aber auch 10-mal mehr Bilder in dem Datensatz vorhanden (s. Kapitel \ref{ch:methodik_datensaetzeCifar10} und \ref{ch:methodik_Tropic}).

Für Finetuning beträgt die Trainingsdauer ungefähr die hälfte der für das Training mit Scratch benötigten Trainingsdauer, da bei Finetune nur für 100 statt 200 Epochen lang trainiert wird (s. Anhang \ref{ch:Anhang_ScatterplotsDauer}).


\begin{figure}[H]
\begin{adjustbox}{width=\textwidth, center}
\includesvg{img/3_R-S-dauer}
\end{adjustbox}
\caption{Scatterplot der benötigten Trainingszeit in Minuten für beide Klassifikationsschemata in allen 3 Frameworks für Resnet Scratch.}
\label{fig:ScatterplotRS-dauer}
\end{figure}

\section{Übersicht über Unterschiede zwischen OvO und OvA}
Da es bei der Vielzahl an Grafiken in Kapitel \ref{ch:ergebnisseOvOOvA} und Anhang \ref{ch:Anhang_ScatterplotsAccuracies} schwierig ist, den Überblick zu behalten und eine gute Gesamtaussage zu treffen, werden in diesem Abschnitt sämtliche Ergebnisse in einem einzigen Graph zusammengefasst dargestellt und beschrieben.\\

In Abbildung \ref{fig:ScatterplotGesamt} befinden sich 3 Graphen, einer je verwendetem Framework.

Jeder dieser Graphen ist durch dicke vertikale Trennlinien in 3 bzw. 2 Abschnitte unterteilt, jeweils ein Abschnitt für ResNet-50, InceptionV3 und zusätzlich InceptionV3-Pawara für die ersten beiden Graphen, da das Netz InceptionV3-Pawara nicht unter PyTorch \cite{pytorch} verwendet wurde.

Wiederum jeder dieser Abschnitte ist in 2 durch eine dünne vertikale Linie voneinander getrennte Bereiche eingeteilt, jeweils einer für Scratch und Finetune.

Auf der Y-Achse wurde die Differenz zwischen den Ergebnissen für OvO und OvA aufgetragen. Dabei wird je Klassifikationsschema über alle 3 bzw. 5 Folds ein Mittelwert gebildet und die Differenz dieser beiden Mittelwerte wird eingezeichnet. Ein Kreuz bei einem positiven Y-Wert von z.B. 2 bedeutet deshalb, dass das OvO Klassifikationsschema gemittelt über alle Folds um 2 Prozentpunkte besser ist, als der OvA Ansatz. Ein Kreuz im negativen Bereich bedeutet daher, dass das OvA Klassifikationsschema besser ist als der OvO Ansatz.
Innerhalb der Scratch bzw. Finetune Bereiche sind die Kreuze nach zunehmender Trainsize sortiert. Es lassen sich somit immer 5 \textit{Säulen} mit Kreuzen innerhalb der Scratch bzw. Finetune Bereiche erkennen, die erste Säule mit allen Ergebnissen mit einer Trainsize von 10 Prozent, die zweite Säule mit 20 Prozent bis hin zu der fünften Säule mit 100 Prozent Trainsize. Mit zunehmender Trainsize werden die Kreuze außerdem farblich heller dargestellt.


Für \textbf{Finetune} (s. Abb. \ref{fig:ScatterplotGesamt}) lässt sich in allen drei Frameworks und Netztypen beobachten, dass die Differenz zwischen OvO und OvA nahezu symmetrisch um $y=0$ verteilt ist und meistens keine allzu große Differenz zwischen den beiden Klassifikationsschemata besteht. Die Streuung der Verteilung um $y=0$ ist also eher gering.
Tendenziell ist die symmetrische Verteilung der Differenzen für Finetune allerdings immer ein bisschen in den negativen Wertebereich verschoben. Dies bedeutet, dass für Finetune der OvA Ansatz insgesamt etwas bessere Ergebnisse erzielt.\\

Für \textbf{Scratch} (s. Abb. \ref{fig:ScatterplotGesamt}) lässt sich besonders für InceptionV3 mit TensorFlow \cite{tensorflow} 1.13.1 und PyTorch eine Verteilung beobachten, die fast ausschließlich im positiven Wertebereich liegt. Auch für die anderen Netztypen bei diesen beiden Frameworks liegt ein Großteil der Kreuze überhalb der X-Achse bei $y=0$. Bei TensorFlow \cite{tensorflow} 2.4.1 befinden sich abgesehen von ResNet ebenfalls deutlich mehr Markierungen oberhalb von $y=0$ als unterhalb, lediglich bei ResNet tritt dort eine näherungsweise symmetrische Verteilung um $y=0$ auf.\\

Es lässt sich somit festhalten, dass der OvO Ansatz für Scratch-Versuche, abgesehen von ResNet in Kombination von TensorFlow \cite{tensorflow} 2.4.1, meistens bessere Accuracies produziert als der OvA Ansatz. Für Finetune Experimente kehrt sich dieser Trend jedoch um und das standardmäßige OvA Klassifikationsschema erzeugt bessere Ergebnisse.

\begin{figure}[H]
\begin{adjustbox}{width=1.4\textwidth, center}
\includesvg{img/3_Differenz-Mittelwerte}
\end{adjustbox}
\caption{Scatterplot der Unterschiede zwischen OvO und OvA gruppiert nach Framework, Netztyp und Gewicht-Initialisierung.}
\label{fig:ScatterplotGesamt}
\end{figure}

\section{Visualisierung des Trainingsverlaufes an einem konkreten Beispiel}
\todo{Beispiel: I-S-Tropic10-TS10}
\todo{Confusion-Matrix für ein Beispiel raussuchen}
\todo{Trainingsverlauf exemplarisch plotten (Accuracy und Loss OvO vs OvA, evtl. für alle verschiedenen Frameworks)}

\section{Zusammenfassung der Ergebnisse}
\todo{Ergebnisse nochmal kurz zusammenfassen, Zurückfinden zur Kernfrage}